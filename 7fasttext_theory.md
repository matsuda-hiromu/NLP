## fasttextの論文を読んだのでまとめておく
- 単語の分散表現に関する論文、テキスト分類に関する論文、二本立てになっている

## 単語の分散表現
- [Enriching Word Vectors with Subword Information　Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov](https://arxiv.org/abs/1607.04606)
- 基本的に単語をngramに分解して、その和を1語としてみている。
- 学習をWord2vecから速くしている
![image](https://user-images.githubusercontent.com/36536038/37946440-516cb970-31c0-11e8-8a61-b4b172759ca7.png)


## テキスト分類論文概要
- [Bag of Tricks for Efficient Text Classification Armand Joulin, Edouard Grave, Piotr Bojanowski, Tomas Mikolov](https://arxiv.org/abs/1607.01759)
- SVMにおけるテキスト分類の課題であった「出力カテゴリーが多用な大規模なコーパスにおいて、低出現頻度カテゴリーを分類できない問題」を解決している
- アルゴリズムはCBoWを応用したもので、「出力：文章ラベル　←入力：f(文章内単語)　」となっている
- Google翻訳にかけて英語と日本語突き合わせて読むだけでも案外簡単に読める
### 序論
- テキスト分類は、Web検索、情報検索、ランキング、文書分類など、多くのアプリケーションで自然言語処理で重要な課題です（Deerwester et al。、1990; Pang and Lee、2008）。
  - 近年、ニューラルネットワークに基づくモデルがますます普及している（Kim、2014; Zhang and LeCun、2015; Conneau et al。、2016）。
  - これらのモデルは実際には非常に優れた性能を達成しますが、列車と試験時間の両方で比較的遅い傾向があり、非常に大きなデータセットでの使用が制限されます。
  - 一方、線形分類器は、テキスト分類の問題のための強力なベースラインと考えられることが多い（Joachims、1998; McCallum and Nigam、1998; Fan et al。、2008）。
  - 彼らのシンプルさにもかかわらず、適切な機能が使用されると、彼らはしばしば最先端のパフォーマンスを得る（Wang and Manning、2012）。
  - また、非常に大きなコーパスに拡大する可能性もあります（Agarwalら、2014）。
- **本論文では、テキスト分類のコンテキストで、これらのベースラインを多様な出力空間を持つ非常に大きなコーパスにスケールする方法を探索します。**
  - 効率的な単語表現学習（Mikolov et al。、2013; Levy et al。、2015）の近年の研究に触発されて、ランク制約と高速損失近似を持つ線形モデルは10分以内に10億語を訓練することができ、 最先端技術と同等の性能を達成しています。
  - 私たちはfastText1のアプローチの品質をタグ予測とセンチメント分析の2つの異なるタスクで評価します。
- 文分類のための単純かつ効率的なベースラインは、文章を単語の袋（BoW）として表現し、ロジスティック回帰やSVMなどの線形分類子を訓練することである（Joachims、1998; Fan et al。、2008）。
  - ただし、線形分類器は、フィーチャとクラス間でパラメータを共有しません。
  - これは、いくつかのクラスが非常に少数の例を有する大きな出力空間の文脈において、それらの一般化を制限する可能性がある。
  - この問題の一般的な解決策は、線形分類器を低ランク行列に分解する（Schutze、1992; Mikolov et al。、2013）か、多層ニューラルネットワークを使用することである（Collobert and Weston、2008; Zhang et al。、2015）。
### モデル概要
- 図1は、ランク制約付きの単純な線形モデルを示しています。
  - 第1の重み行列Aは、単語に対するルックアップテーブルである。
  - その後、単語表現は、平均化されてテキスト表現になり、次にそれが線形分類器に供給される。
  - テキスト表現は、潜在的に再利用される潜在変数である。
  - このアーキテクチャは、Mikolov et al（2013）のcbowモデルに似ています。 中間の単語はラベルに置き換えられます。
  - 事前定義されたクラスにわたる確率分布を計算するためにsoftmax関数fを使用する。
  - N個の文書の集合に対して、これはクラスに対する負の対数を最小限に抑えることにつながります。

![image](https://user-images.githubusercontent.com/36536038/37946195-11e0761c-31bf-11e8-91c8-4cf9463bc668.png)

### 性能

![image](https://user-images.githubusercontent.com/36536038/37945909-9b0c1c9a-31bd-11e8-988f-8b1da41c97ad.png)

![image](https://user-images.githubusercontent.com/36536038/37946183-0373de84-31bf-11e8-9a32-8cbdcb3ba5b1.png)
